{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "STJ50WZPuLw5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## XGB Classifier Vs Gradient Boosting Classifier\n",
        "\n",
        "\n",
        "XGBoost es un algoritmo que ha estado dominando recientemente las competencias de aprendizaje automático aplicado y Kaggle para datos estructurados o tabulares, es una implementación de árboles de decisión mejorados con gradiente diseñados para la velocidad y el rendimiento por lo cual debe ser parte de su kit de herramientas de aprendizaje automático.\n",
        "\n",
        "\n",
        "\n",
        "Gradient Boosting construye un modelo aditivo de forma progresiva hacia el escenario; Permite la optimización de funciones de pérdida diferenciables arbitrarias. En cada etapa n_classes_ regression árboles se ajustan en el gradiente negativo de la función de pérdida de desviación binomial o multinomial. La clasificación binaria es un caso especial en el que solo se induce un único árbol de regresión.\n",
        "\n",
        "\n",
        "Tanto xgboost como gbm siguen el principio de aumento de gradiente. Hay sin embargo, la diferencia en los detalles de modelado. Específicamente, xgboost utilizó una formalización de modelos más regularizada para controlar el ajuste excesivo, lo que le brinda un mejor rendimiento.\n",
        "\n",
        "\n",
        "\n",
        "El nombre xgboost, sin embargo, en realidad se refiere al objetivo de ingeniería para impulsar el límite de recursos de cómputo para algoritmos de árbol potenciados. ¿Cuál es la razón por la que muchas personas utilizan xgboost. Para el modelo, podría ser más adecuado llamarlo como aumento de gradiente regularizado. Tanto xgboost como gbm siguen el principio del aumento de gradiente. \n"
      ]
    }
  ]
}